% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data_import.R
\name{ExtractScholarData}
\alias{ExtractScholarData}
\title{Extract Scholar Data from Google Scholar}
\usage{
ExtractScholarData(
  scholar_id,
  max_publications = 100,
  rate_limit_seconds = 5,
  retry_attempts = 3,
  user_agent = NULL,
  cache_dir = NULL
)
}
\arguments{
\item{scholar_id}{Character string containing the Google Scholar ID (e.g., "qc6CJjYAAAAJ")}

\item{max_publications}{Integer maximum number of publications to retrieve (default: 100)}

\item{rate_limit_seconds}{Numeric seconds to wait between requests (default: 5)}

\item{retry_attempts}{Integer number of retry attempts if request fails (default: 3)}

\item{user_agent}{Character string for custom user agent (optional)}

\item{cache_dir}{Directory for storing cached data (default: NULL for temporary cache)}
}
\value{
ScholarProfile object containing scholar data and publications
}
\description{
Extracts comprehensive citation data for a Google Scholar profile with built-in rate limiting
to prevent blocking. This function addresses the common challenge of Google Scholar blocking
requests when too many are made in a short period.
}
\examples{
\dontrun{
# Extract data for a scholar (replace with actual ID)
scholar_data <- ExtractScholarData("qc6CJjYAAAAJ", max_publications = 50)
}
}
